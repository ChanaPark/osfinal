{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1K0UI_rtE-v6lFMw4INdtsaD43hWIecKe",
      "authorship_tag": "ABX9TyNCWl0d5NCVEDSOJ2Tbqg0I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChanaPark/osfinal/blob/main/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D_%EA%B8%B0%EB%A7%90%EA%B3%BC%EC%A0%9C_%EB%B0%95%EC%B0%AC%EC%95%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7IOLw1nZCr6",
        "outputId": "f4c32d63-a831-40c7-b09e-6548fad634a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.41.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07-Up5zBdOq9",
        "outputId": "62764bd6-97e6-4ac6-bf7e-6e906508f0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 2s\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZkIfIDJdiNO",
        "outputId": "1984d0dc-d3dd-4b1e-9c97-0649277d9e67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B92XyxN5drCT",
        "outputId": "79e84f54-74a7-46fd-beb3-bd7c96e3bb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.204.67.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17tKzJ3pd0Bs",
        "outputId": "4e034ae1-da76-4d8a-9e83-a87db95b67d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEMAj9vzHpEX",
        "outputId": "0a087657-ff70-44d4-c3b3-83732298047c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       77500  0.0  0.0   6484  2276 ?        S    12:28   0:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit cache clear\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5so65lqsd5HW",
        "outputId": "4c9743e8-07c1-49bb-880c-243b470aa42e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.204.67.32:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://early-sides-dress.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<웹크롤링>"
      ],
      "metadata": {
        "id": "2wSQ3sD-eiSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://nct.go.kr/distMental/rating/rating02_2.do'\n",
        "response = requests.get(url)\n",
        "\n",
        "html_content = response.text\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "questions = [td.get_text(strip=True) for td in soup.find_all('td', class_='tLeft')]\n",
        "\n",
        "print(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WWzMfI3eo-B",
        "outputId": "fcac3f5d-f815-44ff-b697-5610039a72e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['일 또는 여가 활동을 하는데 흥미나 즐거움을 느끼지 못함', '기분이 가라앉거나.우울하거나.희망이 없음', '잠이 들거나 계속 잠을 자는 것이 어려움. 또는 잠을 너무 많이 잠', '피곤하다고 느끼거나 기운이 거의 없음', '입맛이 없거나 과식을 함', '자신을 부정적으로 봄 – 혹은 자신이 실패자라고 느끼거나 자신 또는 가족을 실망시킴', '신문을 읽거나 텔레비전 보는 것과 같은 일에 집중하는 것이 어려움', '다른 사람들이 주목할 정도로 너무 느리게 움직이거나 말을 함또는 반대로 평상시보다 많이 움직여서, 너무 안절부절 못하거나 들떠 있음', '자신이 죽는 것이 더 낫다고 생각하거나 어떤 식으로든 자신을 해칠것이라고 생각함']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 질문 데이터를 JSON 파일로 저장\n",
        "data = {\n",
        "    \"questions\": questions\n",
        "}\n",
        "\n",
        "with open(\"questions.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"JSON 파일 저장 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO09P_VahgJn",
        "outputId": "cf162006-978d-4b04-a988-ed0efbb7f598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON 파일 저장 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "app.py 코드\n",
        "\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# API 설정\n",
        "YOUR_API_KEY = '00000'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"상담Chat: the Counselor for You\")\n",
        "\n",
        "# 모델 로드 함수\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    generation_config = genai.GenerationConfig(max_output_tokens=1000)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash', generation_config=generation_config)\n",
        "    st.markdown(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "def load_questions():\n",
        "    with open(\"questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "        return data[\"questions\"]\n",
        "\n",
        "questions = load_questions()\n",
        "total_score = 0\n",
        "\n",
        "def conduct_diagnosis():\n",
        "    global total_score\n",
        "    st.markdown(\"지난 2주 간 다음의 증상에 얼마나 자주 해당되었는지 응답해주세요.\")\n",
        "    st.markdown(\"전혀 해당되지 않았다면 0, 가끔 해당되었다면 1, 7일 이상 해당되었다면 2, 매일 해당되었다면 3을 입력해주세요.\")\n",
        "\n",
        "    for i, question in enumerate(questions):\n",
        "        while True:\n",
        "            answer = input(f\"{i+1}. {question}: \")\n",
        "            if answer in [\"0\", \"1\", \"2\", \"3\"]:\n",
        "                total_score += int(answer)\n",
        "                break\n",
        "            else:\n",
        "              st.markdown(\"0, 1, 2, 3 중 하나를 입력해주세요.\")\n",
        "\n",
        "def provide_feedback(total_score):\n",
        "    if total_score <= 4:\n",
        "        st.markdown(\"총점:\", total_score, \"점. 정상적인 수준입니다.\")\n",
        "    elif total_score <= 9:\n",
        "        st.markdown(\"총점:\", total_score, \"점. 경미한 수준의 우울 증상이 나타날 수 있습니다.\")\n",
        "    elif total_score <= 14:\n",
        "        st.markdown(\"총점:\", total_score, \"점. 중간 수준의 우울 증상이 나타날 수 있습니다.\")\n",
        "    elif total_score <= 19:\n",
        "        st.markdown(\"총점은\", total_score, \"점으로 약간 심한 수준의 우울 증상이 나타날 수 있습니다. 전문가와 상담을 권장합니다.\")\n",
        "    else:\n",
        "        st.markdown(\"총점은\", total_score, \"점으로 아주 심한 수준의 우울 증상이 나타날 수 있습니다. 반드시 전문가와 상담하세요.\")\n",
        "    st.markdown(\"진단을 종료합니다.\")\n",
        "\n",
        "# 대화 세션 초기화 (시스템 메시지 제거)\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(history=[{\n",
        "      \"role\": \"user\", \"parts\": [\"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"]\n",
        "    }])\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.language = \"한국어\"\n",
        "    st.session_state.stage = 1\n",
        "\n",
        "# 시스템 메시지 화면에 출력\n",
        "st.markdown(\"**안녕하세요! 심리적으로 어려운 점이 있으시면 언제든지 말씀해주세요. 함께 해결해 나갈 수 있도록 노력하겠습니다.**\")\n",
        "\n",
        "# 대화 기록 표시\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "if \"diagnosis_started\" not in st.session_state:\n",
        "    st.session_state.diagnosis_started = False\n",
        "\n",
        "# 사용자 입력 처리\n",
        "if prompt := st.chat_input(\"무엇이 고민인가요?\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\", avatar=\"😺\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)\n",
        "    if \"우울\" in prompt:\n",
        "        st.markdown(\"우울증상이 계속된다면 테스트로 자가진단을 실행해보세요.\")\n",
        "        if st.button(\"자가진단 진행\"):\n",
        "            st.session_state.diagnosis_started = True  # 상태 업데이트\n",
        "\n",
        "if st.session_state.diagnosis_started:\n",
        "    try:\n",
        "        st.markdown(\"자가진단을 시작합니다.\")\n",
        "        conduct_diagnosis()\n",
        "        feedback = provide_feedback(total_score)\n",
        "        st.write(feedback)\n",
        "        st.session_state.diagnosis_started = False  # 완료 후 상태 초기화\n",
        "    except Exception as e:\n",
        "        st.error(f\"오류가 발생했습니다: {e}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "MMOKk1uRls5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKnNlGgTxf8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "76ed645c-fd33-45c8-d763-dbbdbd13dee7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport google.generativeai as genai\\nimport streamlit as st\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# API 설정\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"상담Chat: the Counselor for You\")\\n\\n@st.cache_resource\\ndef load_model():\\n    \"\"\"\\n    제미나이 모델을 로드합니다.\\n    \"\"\"\\n    model = genai.GenerativeModel(\\n      model_name=\\'gemini-1.5-chat\\',\\n      temperature=0.5,\\n      max_output_tokens=500)\\n    print(\"모델이 로드되었습니다.\")\\n    return model\\n\\nmodel = load_model()\\n\\n# 대화 세션 초기화\\nif \"chat_session\" not in st.session_state:\\n    st.session_state[\"chat_session\"] = genai.start_chat(\\n        model=\"gemini-1.5-chat\",  # 모델 이름 지정\\n        temperature=0.5,         # 응답의 창의성 제어\\n        max_output_tokens=500,   # 응답의 최대 토큰 수\\n        history=[\\n            {\"role\": \"system\", \"content\": \"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"}\\n        ]\\n    )\\n# 대화 기록 표시\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"상담사\" if content[\\'role\\'] == \"model\" else \"user\"):\\n        st.markdown(content[\\'content\\'])\\n\\n# 사용자 입력 처리\\nif prompt := st.chat_input(\"메시지를 입력하세요.\"):\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"상담사\"):\\n        response = st.session_state.chat_session.send_message(\\n        message=prompt,\\n        temperature=0.5,\\n        max_output_tokens=500\\n        )\\n        st.markdown(response[\\'content\\'])\\n\\n# 웹크롤링 함수\\n@st.cache_resource\\ndef fetch_counseling_articles():\\n    \"\"\"\\n    국립 정신건강 센터에서 심리상담 관련 정보 크롤링\\n    \"\"\"\\n    url = [\"https://nct.go.kr/distMental/rating/rating02_2.do\",\\n            \"https://www.mohw.go.kr/menu.es?mid=a10706040100\",\\n            \"\"\\n    response = requests.get(url)\\n    soup = BeautifulSoup(response.text, \\'html.parser\\')\\n\\n    print(\"심리적인 어려움을 혼자 해결하기 어렵다면 전문가와의 상담을 추천합니다.\")\\n    print(\"국립정신건강센터 등 전문 기관에서 상담을 받아보세요.\")\\n    print(\"더 자세한 정보는 다음 링크에서 확인할 수 있습니다:\", url)\\n\\nif __name__ == \"__main__\":\\n    recommend_professional_counseling()\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "'''\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API 설정\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"상담Chat: the Counselor for You\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    제미나이 모델을 로드합니다.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\n",
        "      model_name='gemini-1.5-chat',\n",
        "      temperature=0.5,\n",
        "      max_output_tokens=500)\n",
        "    print(\"모델이 로드되었습니다.\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# 대화 세션 초기화\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = genai.start_chat(\n",
        "        model=\"gemini-1.5-chat\",  # 모델 이름 지정\n",
        "        temperature=0.5,         # 응답의 창의성 제어\n",
        "        max_output_tokens=500,   # 응답의 최대 토큰 수\n",
        "        history=[\n",
        "            {\"role\": \"system\", \"content\": \"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"}\n",
        "        ]\n",
        "    )\n",
        "# 대화 기록 표시\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"상담사\" if content['role'] == \"model\" else \"user\"):\n",
        "        st.markdown(content['content'])\n",
        "\n",
        "# 사용자 입력 처리\n",
        "if prompt := st.chat_input(\"메시지를 입력하세요.\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"상담사\"):\n",
        "        response = st.session_state.chat_session.send_message(\n",
        "        message=prompt,\n",
        "        temperature=0.5,\n",
        "        max_output_tokens=500\n",
        "        )\n",
        "        st.markdown(response['content'])\n",
        "\n",
        "# 웹크롤링 함수\n",
        "@st.cache_resource\n",
        "def fetch_counseling_articles():\n",
        "    \"\"\"\n",
        "    국립 정신건강 센터에서 심리상담 관련 정보 크롤링\n",
        "    \"\"\"\n",
        "    url = [\"https://nct.go.kr/distMental/rating/rating02_2.do\",\n",
        "            \"https://www.mohw.go.kr/menu.es?mid=a10706040100\",\n",
        "            \"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    print(\"심리적인 어려움을 혼자 해결하기 어렵다면 전문가와의 상담을 추천합니다.\")\n",
        "    print(\"국립정신건강센터 등 전문 기관에서 상담을 받아보세요.\")\n",
        "    print(\"더 자세한 정보는 다음 링크에서 확인할 수 있습니다:\", url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recommend_professional_counseling()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"Gemini-Bot\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    print(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(history=[])\n",
        "\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "if prompt := st.chat_input(\"메시지를 입력하세요.\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)\n",
        "        '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "uu3OhlWlXp07",
        "outputId": "dffdc6b6-926f-4c64-bbce-7754a13dbe03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport google.generativeai as genai\\nimport streamlit as st\\n\\n# genai.configure(api_key=GOOGLE_API_KEY)\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"Gemini-Bot\")\\n\\n@st.cache_resource\\ndef load_model():\\n    model = genai.GenerativeModel(\\'gemini-1.5-flash\\')\\n    print(\"model loaded...\")\\n    return model\\n\\nmodel = load_model()\\n\\nif \"chat_session\" not in st.session_state:    \\n    st.session_state[\"chat_session\"] = model.start_chat(history=[])\\n\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\\n        st.markdown(content.parts[0].text)\\n\\nif prompt := st.chat_input(\"메시지를 입력하세요.\"):    \\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"ai\"):\\n        response = st.session_state.chat_session.send_message(prompt)        \\n        st.markdown(response.text)\\n        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API 설정\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"상담Chat: the Counselor for You\")\n",
        "\n",
        "# 모델 로드 함수\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    generation_config = genai.GenerationConfig(max_output_tokens=500)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash', generation_config=generation_config)\n",
        "    print(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# 대화 세션 초기화\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(\n",
        "        history=[\n",
        "            {\"ai\": \"system\", \"content\": \"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# 대화 기록 표시\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "# 사용자 입력 처리\n",
        "if prompt := st.chat_input(\"무엇이 고민인가요?\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "dAhY4mkVgpFc",
        "outputId": "4fbd1fca-a287-420f-d7d0-01f5be1b5cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import google.generativeai as genai\\nimport streamlit as st\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# API 설정\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"상담Chat: the Counselor for You\")\\n\\n# 모델 로드 함수\\n@st.cache_resource\\ndef load_model():\\n    print(\"model loaded...\")\\n    return genai.GenerativeModel(\\'gemini-1.5-flash\\')\\n\\nmodel = load_model()\\n\\n# 대화 세션 초기화\\nif \"chat_session\" not in st.session_state:\\n    st.session_state[\"chat_session\"] = model.start_chat(\\n        history=[\\n            {\"ai\": \"system\", \"content\": \"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"}\\n        ]\\n    )\\n\\n# 대화 기록 표시\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\\n        st.markdown(content.parts[0].text)\\n\\n# 사용자 입력 처리\\nif prompt := st.chat_input(\"무엇이 고민인가요?\"):\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"ai\"):\\n        response = st.session_state.chat_session.send_message(prompt)\\n        st.markdown(response.text)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#실행됨\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API 설정\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"상담Chat: the Counselor for You\")\n",
        "\n",
        "# 모델 로드 함수\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    generation_config = genai.GenerationConfig(max_output_tokens=500)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash', generation_config=generation_config)\n",
        "    print(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# 대화 세션 초기화 (시스템 메시지 제거)\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(history=[{\n",
        "      \"role\": \"user\", \"parts\": [\"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"]\n",
        "    }])\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.language = \"한국어\"\n",
        "    st.session_state.stage = 1\n",
        "\n",
        "# 시스템 메시지 화면에 출력\n",
        "st.markdown(\"**시스템:** 안녕하세요! 심리적으로 어려운 점이 있으시면 언제든지 말씀해주세요. 함께 해결해 나갈 수 있도록 노력하겠습니다.\")\n",
        "\n",
        "# 대화 기록 표시\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "# 사용자 입력 처리\n",
        "if prompt := st.chat_input(\"무엇이 고민인가요?\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)\n",
        "        '''"
      ],
      "metadata": {
        "id": "VBvgbFUZOY13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}