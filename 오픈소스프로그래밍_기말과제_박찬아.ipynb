{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1K0UI_rtE-v6lFMw4INdtsaD43hWIecKe",
      "authorship_tag": "ABX9TyNCWl0d5NCVEDSOJ2Tbqg0I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChanaPark/osfinal/blob/main/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D_%EA%B8%B0%EB%A7%90%EA%B3%BC%EC%A0%9C_%EB%B0%95%EC%B0%AC%EC%95%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7IOLw1nZCr6",
        "outputId": "f4c32d63-a831-40c7-b09e-6548fad634a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.41.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07-Up5zBdOq9",
        "outputId": "62764bd6-97e6-4ac6-bf7e-6e906508f0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 2s\n",
            "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZkIfIDJdiNO",
        "outputId": "1984d0dc-d3dd-4b1e-9c97-0649277d9e67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B92XyxN5drCT",
        "outputId": "79e84f54-74a7-46fd-beb3-bd7c96e3bb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.204.67.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17tKzJ3pd0Bs",
        "outputId": "4e034ae1-da76-4d8a-9e83-a87db95b67d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEMAj9vzHpEX",
        "outputId": "0a087657-ff70-44d4-c3b3-83732298047c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       77500  0.0  0.0   6484  2276 ?        S    12:28   0:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit cache clear\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5so65lqsd5HW",
        "outputId": "4c9743e8-07c1-49bb-880c-243b470aa42e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.204.67.32:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://early-sides-dress.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ì›¹í¬ë¡¤ë§>"
      ],
      "metadata": {
        "id": "2wSQ3sD-eiSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://nct.go.kr/distMental/rating/rating02_2.do'\n",
        "response = requests.get(url)\n",
        "\n",
        "html_content = response.text\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "questions = [td.get_text(strip=True) for td in soup.find_all('td', class_='tLeft')]\n",
        "\n",
        "print(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WWzMfI3eo-B",
        "outputId": "fcac3f5d-f815-44ff-b697-5610039a72e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ì¼ ë˜ëŠ” ì—¬ê°€ í™œë™ì„ í•˜ëŠ”ë° í¥ë¯¸ë‚˜ ì¦ê±°ì›€ì„ ëŠë¼ì§€ ëª»í•¨', 'ê¸°ë¶„ì´ ê°€ë¼ì•‰ê±°ë‚˜.ìš°ìš¸í•˜ê±°ë‚˜.í¬ë§ì´ ì—†ìŒ', 'ì ì´ ë“¤ê±°ë‚˜ ê³„ì† ì ì„ ìëŠ” ê²ƒì´ ì–´ë ¤ì›€. ë˜ëŠ” ì ì„ ë„ˆë¬´ ë§ì´ ì ', 'í”¼ê³¤í•˜ë‹¤ê³  ëŠë¼ê±°ë‚˜ ê¸°ìš´ì´ ê±°ì˜ ì—†ìŒ', 'ì…ë§›ì´ ì—†ê±°ë‚˜ ê³¼ì‹ì„ í•¨', 'ìì‹ ì„ ë¶€ì •ì ìœ¼ë¡œ ë´„ â€“ í˜¹ì€ ìì‹ ì´ ì‹¤íŒ¨ìë¼ê³  ëŠë¼ê±°ë‚˜ ìì‹  ë˜ëŠ” ê°€ì¡±ì„ ì‹¤ë§ì‹œí‚´', 'ì‹ ë¬¸ì„ ì½ê±°ë‚˜ í…”ë ˆë¹„ì „ ë³´ëŠ” ê²ƒê³¼ ê°™ì€ ì¼ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒì´ ì–´ë ¤ì›€', 'ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì£¼ëª©í•  ì •ë„ë¡œ ë„ˆë¬´ ëŠë¦¬ê²Œ ì›€ì§ì´ê±°ë‚˜ ë§ì„ í•¨ë˜ëŠ” ë°˜ëŒ€ë¡œ í‰ìƒì‹œë³´ë‹¤ ë§ì´ ì›€ì§ì—¬ì„œ, ë„ˆë¬´ ì•ˆì ˆë¶€ì ˆ ëª»í•˜ê±°ë‚˜ ë“¤ë–  ìˆìŒ', 'ìì‹ ì´ ì£½ëŠ” ê²ƒì´ ë” ë‚«ë‹¤ê³  ìƒê°í•˜ê±°ë‚˜ ì–´ë–¤ ì‹ìœ¼ë¡œë“  ìì‹ ì„ í•´ì¹ ê²ƒì´ë¼ê³  ìƒê°í•¨']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# ì§ˆë¬¸ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
        "data = {\n",
        "    \"questions\": questions\n",
        "}\n",
        "\n",
        "with open(\"questions.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO09P_VahgJn",
        "outputId": "cf162006-978d-4b04-a988-ed0efbb7f598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "app.py ì½”ë“œ\n",
        "\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# API ì„¤ì •\n",
        "YOUR_API_KEY = '00000'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"ìƒë‹´Chat: the Counselor for You\")\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    generation_config = genai.GenerationConfig(max_output_tokens=1000)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash', generation_config=generation_config)\n",
        "    st.markdown(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "def load_questions():\n",
        "    with open(\"questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "        return data[\"questions\"]\n",
        "\n",
        "questions = load_questions()\n",
        "total_score = 0\n",
        "\n",
        "def conduct_diagnosis():\n",
        "    global total_score\n",
        "    st.markdown(\"ì§€ë‚œ 2ì£¼ ê°„ ë‹¤ìŒì˜ ì¦ìƒì— ì–¼ë§ˆë‚˜ ìì£¼ í•´ë‹¹ë˜ì—ˆëŠ”ì§€ ì‘ë‹µí•´ì£¼ì„¸ìš”.\")\n",
        "    st.markdown(\"ì „í˜€ í•´ë‹¹ë˜ì§€ ì•Šì•˜ë‹¤ë©´ 0, ê°€ë” í•´ë‹¹ë˜ì—ˆë‹¤ë©´ 1, 7ì¼ ì´ìƒ í•´ë‹¹ë˜ì—ˆë‹¤ë©´ 2, ë§¤ì¼ í•´ë‹¹ë˜ì—ˆë‹¤ë©´ 3ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "    for i, question in enumerate(questions):\n",
        "        while True:\n",
        "            answer = input(f\"{i+1}. {question}: \")\n",
        "            if answer in [\"0\", \"1\", \"2\", \"3\"]:\n",
        "                total_score += int(answer)\n",
        "                break\n",
        "            else:\n",
        "              st.markdown(\"0, 1, 2, 3 ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "def provide_feedback(total_score):\n",
        "    if total_score <= 4:\n",
        "        st.markdown(\"ì´ì :\", total_score, \"ì . ì •ìƒì ì¸ ìˆ˜ì¤€ì…ë‹ˆë‹¤.\")\n",
        "    elif total_score <= 9:\n",
        "        st.markdown(\"ì´ì :\", total_score, \"ì . ê²½ë¯¸í•œ ìˆ˜ì¤€ì˜ ìš°ìš¸ ì¦ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    elif total_score <= 14:\n",
        "        st.markdown(\"ì´ì :\", total_score, \"ì . ì¤‘ê°„ ìˆ˜ì¤€ì˜ ìš°ìš¸ ì¦ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    elif total_score <= 19:\n",
        "        st.markdown(\"ì´ì ì€\", total_score, \"ì ìœ¼ë¡œ ì•½ê°„ ì‹¬í•œ ìˆ˜ì¤€ì˜ ìš°ìš¸ ì¦ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ë¬¸ê°€ì™€ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        st.markdown(\"ì´ì ì€\", total_score, \"ì ìœ¼ë¡œ ì•„ì£¼ ì‹¬í•œ ìˆ˜ì¤€ì˜ ìš°ìš¸ ì¦ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.\")\n",
        "    st.markdown(\"ì§„ë‹¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™” (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œê±°)\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(history=[{\n",
        "      \"role\": \"user\", \"parts\": [\"ë‹¹ì‹ ì€ ì‹¬ë¦¬ì ìœ¼ë¡œ ê³µê°í•˜ê³  í˜„ì‹¤ì ì¸ ëŒ€ì±…ì„ ê¶ë¦¬í•´ì£¼ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ì„¸ìš”.\"]\n",
        "    }])\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.language = \"í•œêµ­ì–´\"\n",
        "    st.session_state.stage = 1\n",
        "\n",
        "# ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™”ë©´ì— ì¶œë ¥\n",
        "st.markdown(\"**ì•ˆë…•í•˜ì„¸ìš”! ì‹¬ë¦¬ì ìœ¼ë¡œ ì–´ë ¤ìš´ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”. í•¨ê»˜ í•´ê²°í•´ ë‚˜ê°ˆ ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤.**\")\n",
        "\n",
        "# ëŒ€í™” ê¸°ë¡ í‘œì‹œ\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "if \"diagnosis_started\" not in st.session_state:\n",
        "    st.session_state.diagnosis_started = False\n",
        "\n",
        "# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
        "if prompt := st.chat_input(\"ë¬´ì—‡ì´ ê³ ë¯¼ì¸ê°€ìš”?\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\", avatar=\"ğŸ˜º\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)\n",
        "    if \"ìš°ìš¸\" in prompt:\n",
        "        st.markdown(\"ìš°ìš¸ì¦ìƒì´ ê³„ì†ëœë‹¤ë©´ í…ŒìŠ¤íŠ¸ë¡œ ìê°€ì§„ë‹¨ì„ ì‹¤í–‰í•´ë³´ì„¸ìš”.\")\n",
        "        if st.button(\"ìê°€ì§„ë‹¨ ì§„í–‰\"):\n",
        "            st.session_state.diagnosis_started = True  # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "\n",
        "if st.session_state.diagnosis_started:\n",
        "    try:\n",
        "        st.markdown(\"ìê°€ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "        conduct_diagnosis()\n",
        "        feedback = provide_feedback(total_score)\n",
        "        st.write(feedback)\n",
        "        st.session_state.diagnosis_started = False  # ì™„ë£Œ í›„ ìƒíƒœ ì´ˆê¸°í™”\n",
        "    except Exception as e:\n",
        "        st.error(f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "MMOKk1uRls5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKnNlGgTxf8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "76ed645c-fd33-45c8-d763-dbbdbd13dee7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport google.generativeai as genai\\nimport streamlit as st\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# API ì„¤ì •\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"ìƒë‹´Chat: the Counselor for You\")\\n\\n@st.cache_resource\\ndef load_model():\\n    \"\"\"\\n    ì œë¯¸ë‚˜ì´ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\\n    \"\"\"\\n    model = genai.GenerativeModel(\\n      model_name=\\'gemini-1.5-chat\\',\\n      temperature=0.5,\\n      max_output_tokens=500)\\n    print(\"ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\\n    return model\\n\\nmodel = load_model()\\n\\n# ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™”\\nif \"chat_session\" not in st.session_state:\\n    st.session_state[\"chat_session\"] = genai.start_chat(\\n        model=\"gemini-1.5-chat\",  # ëª¨ë¸ ì´ë¦„ ì§€ì •\\n        temperature=0.5,         # ì‘ë‹µì˜ ì°½ì˜ì„± ì œì–´\\n        max_output_tokens=500,   # ì‘ë‹µì˜ ìµœëŒ€ í† í° ìˆ˜\\n        history=[\\n            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì‹¬ë¦¬ì ìœ¼ë¡œ ê³µê°í•˜ê³  í˜„ì‹¤ì ì¸ ëŒ€ì±…ì„ ê¶ë¦¬í•´ì£¼ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ì„¸ìš”.\"}\\n        ]\\n    )\\n# ëŒ€í™” ê¸°ë¡ í‘œì‹œ\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"ìƒë‹´ì‚¬\" if content[\\'role\\'] == \"model\" else \"user\"):\\n        st.markdown(content[\\'content\\'])\\n\\n# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\\nif prompt := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"):\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"ìƒë‹´ì‚¬\"):\\n        response = st.session_state.chat_session.send_message(\\n        message=prompt,\\n        temperature=0.5,\\n        max_output_tokens=500\\n        )\\n        st.markdown(response[\\'content\\'])\\n\\n# ì›¹í¬ë¡¤ë§ í•¨ìˆ˜\\n@st.cache_resource\\ndef fetch_counseling_articles():\\n    \"\"\"\\n    êµ­ë¦½ ì •ì‹ ê±´ê°• ì„¼í„°ì—ì„œ ì‹¬ë¦¬ìƒë‹´ ê´€ë ¨ ì •ë³´ í¬ë¡¤ë§\\n    \"\"\"\\n    url = [\"https://nct.go.kr/distMental/rating/rating02_2.do\",\\n            \"https://www.mohw.go.kr/menu.es?mid=a10706040100\",\\n            \"\"\\n    response = requests.get(url)\\n    soup = BeautifulSoup(response.text, \\'html.parser\\')\\n\\n    print(\"ì‹¬ë¦¬ì ì¸ ì–´ë ¤ì›€ì„ í˜¼ì í•´ê²°í•˜ê¸° ì–´ë µë‹¤ë©´ ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì„ ì¶”ì²œí•©ë‹ˆë‹¤.\")\\n    print(\"êµ­ë¦½ì •ì‹ ê±´ê°•ì„¼í„° ë“± ì „ë¬¸ ê¸°ê´€ì—ì„œ ìƒë‹´ì„ ë°›ì•„ë³´ì„¸ìš”.\")\\n    print(\"ë” ìì„¸í•œ ì •ë³´ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\", url)\\n\\nif __name__ == \"__main__\":\\n    recommend_professional_counseling()\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "'''\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API ì„¤ì •\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"ìƒë‹´Chat: the Counselor for You\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    ì œë¯¸ë‚˜ì´ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\n",
        "      model_name='gemini-1.5-chat',\n",
        "      temperature=0.5,\n",
        "      max_output_tokens=500)\n",
        "    print(\"ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™”\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = genai.start_chat(\n",
        "        model=\"gemini-1.5-chat\",  # ëª¨ë¸ ì´ë¦„ ì§€ì •\n",
        "        temperature=0.5,         # ì‘ë‹µì˜ ì°½ì˜ì„± ì œì–´\n",
        "        max_output_tokens=500,   # ì‘ë‹µì˜ ìµœëŒ€ í† í° ìˆ˜\n",
        "        history=[\n",
        "            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì‹¬ë¦¬ì ìœ¼ë¡œ ê³µê°í•˜ê³  í˜„ì‹¤ì ì¸ ëŒ€ì±…ì„ ê¶ë¦¬í•´ì£¼ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ì„¸ìš”.\"}\n",
        "        ]\n",
        "    )\n",
        "# ëŒ€í™” ê¸°ë¡ í‘œì‹œ\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ìƒë‹´ì‚¬\" if content['role'] == \"model\" else \"user\"):\n",
        "        st.markdown(content['content'])\n",
        "\n",
        "# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
        "if prompt := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ìƒë‹´ì‚¬\"):\n",
        "        response = st.session_state.chat_session.send_message(\n",
        "        message=prompt,\n",
        "        temperature=0.5,\n",
        "        max_output_tokens=500\n",
        "        )\n",
        "        st.markdown(response['content'])\n",
        "\n",
        "# ì›¹í¬ë¡¤ë§ í•¨ìˆ˜\n",
        "@st.cache_resource\n",
        "def fetch_counseling_articles():\n",
        "    \"\"\"\n",
        "    êµ­ë¦½ ì •ì‹ ê±´ê°• ì„¼í„°ì—ì„œ ì‹¬ë¦¬ìƒë‹´ ê´€ë ¨ ì •ë³´ í¬ë¡¤ë§\n",
        "    \"\"\"\n",
        "    url = [\"https://nct.go.kr/distMental/rating/rating02_2.do\",\n",
        "            \"https://www.mohw.go.kr/menu.es?mid=a10706040100\",\n",
        "            \"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    print(\"ì‹¬ë¦¬ì ì¸ ì–´ë ¤ì›€ì„ í˜¼ì í•´ê²°í•˜ê¸° ì–´ë µë‹¤ë©´ ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì„ ì¶”ì²œí•©ë‹ˆë‹¤.\")\n",
        "    print(\"êµ­ë¦½ì •ì‹ ê±´ê°•ì„¼í„° ë“± ì „ë¬¸ ê¸°ê´€ì—ì„œ ìƒë‹´ì„ ë°›ì•„ë³´ì„¸ìš”.\")\n",
        "    print(\"ë” ìì„¸í•œ ì •ë³´ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\", url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recommend_professional_counseling()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"Gemini-Bot\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    print(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(history=[])\n",
        "\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "if prompt := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)\n",
        "        '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "uu3OhlWlXp07",
        "outputId": "dffdc6b6-926f-4c64-bbce-7754a13dbe03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport google.generativeai as genai\\nimport streamlit as st\\n\\n# genai.configure(api_key=GOOGLE_API_KEY)\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"Gemini-Bot\")\\n\\n@st.cache_resource\\ndef load_model():\\n    model = genai.GenerativeModel(\\'gemini-1.5-flash\\')\\n    print(\"model loaded...\")\\n    return model\\n\\nmodel = load_model()\\n\\nif \"chat_session\" not in st.session_state:    \\n    st.session_state[\"chat_session\"] = model.start_chat(history=[])\\n\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\\n        st.markdown(content.parts[0].text)\\n\\nif prompt := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"):    \\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"ai\"):\\n        response = st.session_state.chat_session.send_message(prompt)        \\n        st.markdown(response.text)\\n        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API ì„¤ì •\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"ìƒë‹´Chat: the Counselor for You\")\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    generation_config = genai.GenerationConfig(max_output_tokens=500)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash', generation_config=generation_config)\n",
        "    print(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™”\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(\n",
        "        history=[\n",
        "            {\"ai\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì‹¬ë¦¬ì ìœ¼ë¡œ ê³µê°í•˜ê³  í˜„ì‹¤ì ì¸ ëŒ€ì±…ì„ ê¶ë¦¬í•´ì£¼ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ì„¸ìš”.\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# ëŒ€í™” ê¸°ë¡ í‘œì‹œ\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
        "if prompt := st.chat_input(\"ë¬´ì—‡ì´ ê³ ë¯¼ì¸ê°€ìš”?\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "dAhY4mkVgpFc",
        "outputId": "4fbd1fca-a287-420f-d7d0-01f5be1b5cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import google.generativeai as genai\\nimport streamlit as st\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# API ì„¤ì •\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"ìƒë‹´Chat: the Counselor for You\")\\n\\n# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\\n@st.cache_resource\\ndef load_model():\\n    print(\"model loaded...\")\\n    return genai.GenerativeModel(\\'gemini-1.5-flash\\')\\n\\nmodel = load_model()\\n\\n# ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™”\\nif \"chat_session\" not in st.session_state:\\n    st.session_state[\"chat_session\"] = model.start_chat(\\n        history=[\\n            {\"ai\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì‹¬ë¦¬ì ìœ¼ë¡œ ê³µê°í•˜ê³  í˜„ì‹¤ì ì¸ ëŒ€ì±…ì„ ê¶ë¦¬í•´ì£¼ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ì„¸ìš”.\"}\\n        ]\\n    )\\n\\n# ëŒ€í™” ê¸°ë¡ í‘œì‹œ\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\\n        st.markdown(content.parts[0].text)\\n\\n# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\\nif prompt := st.chat_input(\"ë¬´ì—‡ì´ ê³ ë¯¼ì¸ê°€ìš”?\"):\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"ai\"):\\n        response = st.session_state.chat_session.send_message(prompt)\\n        st.markdown(response.text)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#ì‹¤í–‰ë¨\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API ì„¤ì •\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"ìƒë‹´Chat: the Counselor for You\")\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    generation_config = genai.GenerationConfig(max_output_tokens=500)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash', generation_config=generation_config)\n",
        "    print(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™” (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œê±°)\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(history=[{\n",
        "      \"role\": \"user\", \"parts\": [\"ë‹¹ì‹ ì€ ì‹¬ë¦¬ì ìœ¼ë¡œ ê³µê°í•˜ê³  í˜„ì‹¤ì ì¸ ëŒ€ì±…ì„ ê¶ë¦¬í•´ì£¼ëŠ” ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ì„¸ìš”.\"]\n",
        "    }])\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.language = \"í•œêµ­ì–´\"\n",
        "    st.session_state.stage = 1\n",
        "\n",
        "# ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™”ë©´ì— ì¶œë ¥\n",
        "st.markdown(\"**ì‹œìŠ¤í…œ:** ì•ˆë…•í•˜ì„¸ìš”! ì‹¬ë¦¬ì ìœ¼ë¡œ ì–´ë ¤ìš´ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”. í•¨ê»˜ í•´ê²°í•´ ë‚˜ê°ˆ ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ëŒ€í™” ê¸°ë¡ í‘œì‹œ\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬\n",
        "if prompt := st.chat_input(\"ë¬´ì—‡ì´ ê³ ë¯¼ì¸ê°€ìš”?\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)\n",
        "        '''"
      ],
      "metadata": {
        "id": "VBvgbFUZOY13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}