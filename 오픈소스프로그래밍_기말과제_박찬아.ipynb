{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1K0UI_rtE-v6lFMw4INdtsaD43hWIecKe",
      "authorship_tag": "ABX9TyNRdUDQoJFvVEEexrC8ljr5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChanaPark/osfinal/blob/main/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D_%EA%B8%B0%EB%A7%90%EA%B3%BC%EC%A0%9C_%EB%B0%95%EC%B0%AC%EC%95%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7IOLw1nZCr6",
        "outputId": "f4c32d63-a831-40c7-b09e-6548fad634a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.41.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07-Up5zBdOq9",
        "outputId": "62764bd6-97e6-4ac6-bf7e-6e906508f0d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 2s\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZkIfIDJdiNO",
        "outputId": "2818a50e-2cf1-4aad-94cd-45ef608f7474"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B92XyxN5drCT",
        "outputId": "79e84f54-74a7-46fd-beb3-bd7c96e3bb9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.204.67.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17tKzJ3pd0Bs",
        "outputId": "4e034ae1-da76-4d8a-9e83-a87db95b67d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEMAj9vzHpEX",
        "outputId": "0a087657-ff70-44d4-c3b3-83732298047c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       77500  0.0  0.0   6484  2276 ?        S    12:28   0:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit cache clear\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5so65lqsd5HW",
        "outputId": "7266a07e-03f8-4311-a0a4-9623c2d32248"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.204.67.32:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://tame-eagles-mix.loca.lt\n",
            "model loaded...\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1734786491.460124   87047 init.cc:229] grpc_wait_for_shutdown_with_timeout() timed out.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WKnNlGgTxf8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "76ed645c-fd33-45c8-d763-dbbdbd13dee7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport google.generativeai as genai\\nimport streamlit as st\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# API 설정\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"상담Chat: the Counselor for You\")\\n\\n@st.cache_resource\\ndef load_model():\\n    \"\"\"\\n    제미나이 모델을 로드합니다.\\n    \"\"\"\\n    model = genai.GenerativeModel(\\n      model_name=\\'gemini-1.5-chat\\',\\n      temperature=0.5,\\n      max_output_tokens=500)\\n    print(\"모델이 로드되었습니다.\")\\n    return model\\n\\nmodel = load_model()\\n\\n# 대화 세션 초기화\\nif \"chat_session\" not in st.session_state:\\n    st.session_state[\"chat_session\"] = genai.start_chat(\\n        model=\"gemini-1.5-chat\",  # 모델 이름 지정\\n        temperature=0.5,         # 응답의 창의성 제어\\n        max_output_tokens=500,   # 응답의 최대 토큰 수\\n        history=[\\n            {\"role\": \"system\", \"content\": \"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"}\\n        ]\\n    )\\n# 대화 기록 표시\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"상담사\" if content[\\'role\\'] == \"model\" else \"user\"):\\n        st.markdown(content[\\'content\\'])\\n\\n# 사용자 입력 처리\\nif prompt := st.chat_input(\"메시지를 입력하세요.\"):\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"상담사\"):\\n        response = st.session_state.chat_session.send_message(\\n        message=prompt,\\n        temperature=0.5,\\n        max_output_tokens=500\\n        )\\n        st.markdown(response[\\'content\\'])\\n\\n# 웹크롤링 함수\\n@st.cache_resource\\ndef fetch_counseling_articles():\\n    \"\"\"\\n    국립 정신건강 센터에서 심리상담 관련 정보 크롤링\\n    \"\"\"\\n    url = [\"https://nct.go.kr/distMental/rating/rating02_2.do\",\\n            \"https://www.mohw.go.kr/menu.es?mid=a10706040100\",\\n            \"\"\\n    response = requests.get(url)\\n    soup = BeautifulSoup(response.text, \\'html.parser\\')\\n\\n    print(\"심리적인 어려움을 혼자 해결하기 어렵다면 전문가와의 상담을 추천합니다.\")\\n    print(\"국립정신건강센터 등 전문 기관에서 상담을 받아보세요.\")\\n    print(\"더 자세한 정보는 다음 링크에서 확인할 수 있습니다:\", url)\\n\\nif __name__ == \"__main__\":\\n    recommend_professional_counseling()\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "'''\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API 설정\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"상담Chat: the Counselor for You\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    제미나이 모델을 로드합니다.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\n",
        "      model_name='gemini-1.5-chat',\n",
        "      temperature=0.5,\n",
        "      max_output_tokens=500)\n",
        "    print(\"모델이 로드되었습니다.\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# 대화 세션 초기화\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = genai.start_chat(\n",
        "        model=\"gemini-1.5-chat\",  # 모델 이름 지정\n",
        "        temperature=0.5,         # 응답의 창의성 제어\n",
        "        max_output_tokens=500,   # 응답의 최대 토큰 수\n",
        "        history=[\n",
        "            {\"role\": \"system\", \"content\": \"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"}\n",
        "        ]\n",
        "    )\n",
        "# 대화 기록 표시\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"상담사\" if content['role'] == \"model\" else \"user\"):\n",
        "        st.markdown(content['content'])\n",
        "\n",
        "# 사용자 입력 처리\n",
        "if prompt := st.chat_input(\"메시지를 입력하세요.\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"상담사\"):\n",
        "        response = st.session_state.chat_session.send_message(\n",
        "        message=prompt,\n",
        "        temperature=0.5,\n",
        "        max_output_tokens=500\n",
        "        )\n",
        "        st.markdown(response['content'])\n",
        "\n",
        "# 웹크롤링 함수\n",
        "@st.cache_resource\n",
        "def fetch_counseling_articles():\n",
        "    \"\"\"\n",
        "    국립 정신건강 센터에서 심리상담 관련 정보 크롤링\n",
        "    \"\"\"\n",
        "    url = [\"https://nct.go.kr/distMental/rating/rating02_2.do\",\n",
        "            \"https://www.mohw.go.kr/menu.es?mid=a10706040100\",\n",
        "            \"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    print(\"심리적인 어려움을 혼자 해결하기 어렵다면 전문가와의 상담을 추천합니다.\")\n",
        "    print(\"국립정신건강센터 등 전문 기관에서 상담을 받아보세요.\")\n",
        "    print(\"더 자세한 정보는 다음 링크에서 확인할 수 있습니다:\", url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recommend_professional_counseling()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"Gemini-Bot\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    print(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(history=[])\n",
        "\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "if prompt := st.chat_input(\"메시지를 입력하세요.\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)\n",
        "        '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "uu3OhlWlXp07",
        "outputId": "dffdc6b6-926f-4c64-bbce-7754a13dbe03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport google.generativeai as genai\\nimport streamlit as st\\n\\n# genai.configure(api_key=GOOGLE_API_KEY)\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"Gemini-Bot\")\\n\\n@st.cache_resource\\ndef load_model():\\n    model = genai.GenerativeModel(\\'gemini-1.5-flash\\')\\n    print(\"model loaded...\")\\n    return model\\n\\nmodel = load_model()\\n\\nif \"chat_session\" not in st.session_state:    \\n    st.session_state[\"chat_session\"] = model.start_chat(history=[])\\n\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\\n        st.markdown(content.parts[0].text)\\n\\nif prompt := st.chat_input(\"메시지를 입력하세요.\"):    \\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"ai\"):\\n        response = st.session_state.chat_session.send_message(prompt)        \\n        st.markdown(response.text)\\n        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API 설정\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"상담Chat: the Counselor for You\")\n",
        "\n",
        "# 모델 로드 함수\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    print(\"model loaded...\")\n",
        "    return genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# 대화 세션 초기화\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(\n",
        "        history=[\n",
        "            {\"ai\": \"system\", \"content\": \"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# 대화 기록 표시\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "# 사용자 입력 처리\n",
        "if prompt := st.chat_input(\"무엇이 고민인가요?\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "dAhY4mkVgpFc",
        "outputId": "4fbd1fca-a287-420f-d7d0-01f5be1b5cbd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import google.generativeai as genai\\nimport streamlit as st\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# API 설정\\nYOUR_API_KEY = \\'key\\'\\ngenai.configure(api_key=YOUR_API_KEY)\\n\\nst.title(\"상담Chat: the Counselor for You\")\\n\\n# 모델 로드 함수\\n@st.cache_resource\\ndef load_model():\\n    print(\"model loaded...\")\\n    return genai.GenerativeModel(\\'gemini-1.5-flash\\')\\n\\nmodel = load_model()\\n\\n# 대화 세션 초기화\\nif \"chat_session\" not in st.session_state:\\n    st.session_state[\"chat_session\"] = model.start_chat(\\n        history=[\\n            {\"ai\": \"system\", \"content\": \"당신은 심리적으로 공감하고 현실적인 대책을 궁리해주는 상담사입니다. 사용자에게 도움을 주세요.\"}\\n        ]\\n    )\\n\\n# 대화 기록 표시\\nfor content in st.session_state.chat_session.history:\\n    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\\n        st.markdown(content.parts[0].text)\\n\\n# 사용자 입력 처리\\nif prompt := st.chat_input(\"무엇이 고민인가요?\"):\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    with st.chat_message(\"ai\"):\\n        response = st.session_state.chat_session.send_message(prompt)\\n        st.markdown(response.text)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#실행됨\n",
        "import google.generativeai as genai\n",
        "import streamlit as st\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# API 설정\n",
        "YOUR_API_KEY = 'key'\n",
        "genai.configure(api_key=YOUR_API_KEY)\n",
        "\n",
        "st.title(\"상담Chat: the Counselor for You\")\n",
        "\n",
        "# 모델 로드 함수\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    print(\"model loaded...\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# 대화 세션 초기화 (시스템 메시지 제거)\n",
        "if \"chat_session\" not in st.session_state:\n",
        "    st.session_state[\"chat_session\"] = model.start_chat(history=[])\n",
        "\n",
        "# 시스템 메시지 화면에 출력\n",
        "st.markdown(\"**시스템:** 안녕하세요! 심리적으로 어려운 점이 있으시면 언제든지 말씀해주세요. 함께 해결해 나갈 수 있도록 노력하겠습니다.\")\n",
        "\n",
        "# 대화 기록 표시\n",
        "for content in st.session_state.chat_session.history:\n",
        "    with st.chat_message(\"ai\" if content.role == \"model\" else \"user\"):\n",
        "        st.markdown(content.parts[0].text)\n",
        "\n",
        "# 사용자 입력 처리\n",
        "if prompt := st.chat_input(\"무엇이 고민인가요?\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = st.session_state.chat_session.send_message(prompt)\n",
        "        st.markdown(response.text)\n",
        "        '''"
      ],
      "metadata": {
        "id": "VBvgbFUZOY13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}